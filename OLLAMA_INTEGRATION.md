# Ollama Integration Guide

## ü§ñ **Enhanced AI System with Fallback Support**

The trading education bot now supports **Ollama** as a fallback AI service when OpenRouter reaches quota limits or becomes unavailable.

### üöÄ **How It Works**

1. **Primary**: Tries OpenRouter first (if `OPENROUTER_API_KEY` is set)
2. **Fallback**: Automatically switches to Ollama if OpenRouter fails
3. **Intelligent**: Detects quota limits, timeouts, and rate limits
4. **Seamless**: No manual intervention required

### ‚öôÔ∏è **Configuration**

Add these environment variables to your Cloudflare Worker:

```bash
# Ollama Configuration
OLLAMA_API_URL=http://localhost:11434  # Your Ollama server URL
OLLAMA_API_KEY=your-api-key            # Optional: if auth required
OLLAMA_MODEL=gpt-oss:latest                  # Optional: default model
```

### üîß **Ollama Setup Options**

#### **Option 1: Local Ollama Server**
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama3.2
ollama pull phi3
ollama pull mistral

# Start server (runs on localhost:11434)
ollama serve
```

#### **Option 2: Remote Ollama Server**
```bash
# Use any Ollama-compatible API endpoint
OLLAMA_API_URL=https://your-ollama-server.com/api
```

#### **Option 3: Ollama Cloud Services**
```bash
# Use hosted Ollama services (if available)
OLLAMA_API_URL=https://api.ollama-provider.com
OLLAMA_API_KEY=your-cloud-api-key
```

### üìä **Supported Models**

Common Ollama models for trading content:

| Model | Size | Quality | Speed | Best For |
|-------|------|---------|-------|----------|
| `llama3.2` | 2GB | High | Fast | General education |
| `phi3` | 2.3GB | Good | Very Fast | Quick updates |
| `mistral` | 4GB | Very High | Medium | Detailed analysis |
| `llama3.1:8b` | 4.7GB | Excellent | Medium | Professional content |
| `codellama` | 3.8GB | High | Fast | Technical analysis |

### üéØ **Usage Examples**

#### **Environment Variables Setup**
```toml
# wrangler.toml or environment
[vars]
OPENROUTER_API_KEY = "sk-or-your-key"     # Primary
OLLAMA_API_URL = "http://localhost:11434"  # Fallback
OLLAMA_MODEL = "gpt-oss:latest"                  # Model choice
```

#### **Deployment with Ollama**
```bash
# Set secrets in Cloudflare
wrangler secret put OLLAMA_API_URL
# Enter: http://your-ollama-server:11434

wrangler secret put OLLAMA_MODEL  
# Enter: llama3.2

# Deploy with fallback support
npm run deploy
```

### üìà **Benefits**

#### **Reliability**
- ‚úÖ **99% uptime** - Automatic fallback ensures posts continue
- ‚úÖ **No quota limits** - Local Ollama has no usage restrictions
- ‚úÖ **Cost control** - Reduce OpenRouter usage automatically

#### **Performance**  
- ‚úÖ **Smart routing** - Uses fastest available service
- ‚úÖ **Timeout handling** - 30s OpenRouter, 45s Ollama limits
- ‚úÖ **Error detection** - Intelligent service switching

#### **Privacy**
- ‚úÖ **Local processing** - Keep sensitive prompts on your server
- ‚úÖ **No external calls** - Ollama can run completely offline
- ‚úÖ **Data control** - Full control over AI processing

### üîç **Monitoring & Debugging**

#### **Admin Dashboard**
The web interface now shows:
- ‚úÖ **OpenRouter Status** - API key and availability
- ‚úÖ **Ollama Status** - URL and connection status  
- ‚úÖ **Current AI Source** - Which service generated content
- ‚úÖ **Fallback Events** - When switching occurs

#### **Logs & Diagnostics**
```bash
# Check real-time logs
npx wrangler tail

# Look for these messages:
# ‚úÖ OpenRouter generation successful
# üîÑ OpenRouter quota exceeded, trying Ollama fallback...
# ‚úÖ Ollama generation successful
```

### ‚ö° **Quick Start with Docker**

```bash
# Run Ollama in Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull a model
docker exec -it ollama ollama pull llama3.2

# Set in Cloudflare Worker
OLLAMA_API_URL=http://your-docker-host:11434
```

### üõ†Ô∏è **Troubleshooting**

#### **Common Issues**

| Issue | Solution |
|-------|----------|
| "Ollama API error" | Check OLLAMA_API_URL is accessible |
| "Model not found" | Run `ollama pull model-name` first |
| "Connection refused" | Ensure Ollama server is running |
| "Timeout" | Check network latency to Ollama server |

#### **Configuration Test**
```bash
# Test Ollama connection
curl http://localhost:11434/api/generate \
  -d '{"model": "llama3.2", "prompt": "Test", "stream": false}'
```

### üìö **Advanced Configuration**

#### **Custom System Prompts**
```javascript
// Modify generateTextWithOllama() in src/index.js
const systemPrompt = `Your custom trading education prompt...`;
```

#### **Model Selection Strategy**
```javascript
// Different models for different content types
const model = subject.includes('analysis') ? 'mistral' : 'llama3.2';
```

#### **Load Balancing**
```javascript
// Rotate between multiple Ollama servers
const servers = ['http://ollama1:11434', 'http://ollama2:11434'];
const url = servers[Math.floor(Math.random() * servers.length)];
```

### üéØ **Best Practices**

1. **Use OpenRouter for premium content** - Better for complex analysis
2. **Use Ollama for high-frequency posts** - No quota concerns
3. **Monitor both services** - Set up health checks
4. **Keep models updated** - Regular `ollama pull` updates
5. **Backup configurations** - Document your setup

### üöÄ **Future Enhancements**

- [ ] Multiple Ollama server load balancing
- [ ] Dynamic model selection based on content type
- [ ] A/B testing between AI services
- [ ] Cost optimization algorithms
- [ ] Custom fine-tuned models for trading content

---

**Your trading bot is now bulletproof with dual AI support!** üéØ